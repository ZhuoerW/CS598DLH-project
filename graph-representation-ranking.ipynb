{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARS-CoV-2 Knowledge Graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please Change the Flag Below to Change the Embedding Algorithm used.\n",
    "\n",
    "#### DRKG = True will only use DRKG embedding\n",
    "#### DRKG = False will use the DRKG + SARS-CoV-2 knowledge graph embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRGK = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.modules import Module\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.utils import add_remaining_self_loops, add_self_loops\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.nn import GCNConv, SAGEConv,GAE, VGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path='data/'\n",
    "exp_id='vDRKG'\n",
    "device_id='cpu' #'cpu' if CPU, device number if GPU\n",
    "embedding_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=pickle.load(open(data_path+'LabelEncoder_'+exp_id+'.pkl', 'rb'))\n",
    "edge_index=pickle.load(open(data_path+'edge_index_'+exp_id+'.pkl','rb'))\n",
    "node_feature_np=pickle.load(open(data_path+'node_feature_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feature=torch.tensor(node_feature_np, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge=torch.tensor(edge_index[['node1', 'node2']].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr_dict={'gene-drug':0,'gene-gene':1,'bait-gene':2, 'gene-phenotype':3, 'drug-phenotype':4}\n",
    "edge_index['type']=edge_index['type'].apply(lambda x: edge_attr_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    29005\n",
       "1     6174\n",
       "3     2053\n",
       "4     1365\n",
       "2      247\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr=torch.tensor(edge_index['type'].values,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=node_feature,\n",
    "            edge_index=edge.t().contiguous(),\n",
    "            edge_attr=edge_attr\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 15700, 38844)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.num_features, data.num_nodes,data.num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38844])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_attr.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.has_isolated_nodes(), data.is_directed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_edges(data, val_ratio=0.05, test_ratio=0.1):\n",
    "    r\"\"\"Splits the edges of a :obj:`torch_geometric.data.Data` object\n",
    "    into positive and negative train/val/test edges, and adds attributes of\n",
    "    `train_pos_edge_index`, `train_neg_adj_mask`, `val_pos_edge_index`,\n",
    "    `val_neg_edge_index`, `test_pos_edge_index`, and `test_neg_edge_index`\n",
    "    to :attr:`data`.\n",
    "\n",
    "    Args:\n",
    "        data (Data): The data object.\n",
    "        val_ratio (float, optional): The ratio of positive validation\n",
    "            edges. (default: :obj:`0.05`)\n",
    "        test_ratio (float, optional): The ratio of positive test\n",
    "            edges. (default: :obj:`0.1`)\n",
    "\n",
    "    :rtype: :class:`torch_geometric.data.Data`\n",
    "    \"\"\"\n",
    "\n",
    "    assert 'batch' not in data  # No batch-mode.\n",
    "\n",
    "    num_nodes = data.num_nodes\n",
    "    row, col = data.edge_index\n",
    "    #data.edge_index = None\n",
    "    attr = data.edge_attr\n",
    "\n",
    "    # Return upper triangular portion.\n",
    "    #mask = row < col\n",
    "    #row, col = row[mask], col[mask]\n",
    "\n",
    "    n_v = int(math.floor(val_ratio * row.size(0)))\n",
    "    n_t = int(math.floor(test_ratio * row.size(0)))\n",
    "\n",
    "    # Positive edges.\n",
    "    perm = torch.randperm(row.size(0))\n",
    "    row, col = row[perm], col[perm]\n",
    "    attr=attr[perm]\n",
    "\n",
    "    r, c = row[:n_v], col[:n_v]\n",
    "    data.val_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "    data.val_pos_edge_attr = attr[:n_v]\n",
    "    \n",
    "    r, c = row[n_v:n_v + n_t], col[n_v:n_v + n_t]\n",
    "    data.test_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "    data.test_post_edge_attr = attr[n_v:n_v + n_t]\n",
    "\n",
    "    r, c = row[n_v + n_t:], col[n_v + n_t:]\n",
    "    data.train_pos_edge_index = torch.stack([r, c], dim=0)\n",
    "    data.train_pos_edge_attr = attr[n_v+n_t:]\n",
    "\n",
    "    # Negative edges.\n",
    "    neg_adj_mask = torch.ones(num_nodes, num_nodes, dtype=torch.uint8)\n",
    "    neg_adj_mask = neg_adj_mask.triu(diagonal=1).to(torch.bool)\n",
    "    neg_adj_mask[row, col] = 0\n",
    "\n",
    "    neg_row, neg_col = neg_adj_mask.nonzero().t()\n",
    "    perm = random.sample(range(neg_row.size(0)),\n",
    "                         min(n_v + n_t, neg_row.size(0)))\n",
    "    perm = torch.tensor(perm)\n",
    "    perm = perm.to(torch.long)\n",
    "    neg_row, neg_col = neg_row[perm], neg_col[perm]\n",
    "\n",
    "    neg_adj_mask[neg_row, neg_col] = 0\n",
    "    data.train_neg_adj_mask = neg_adj_mask\n",
    "\n",
    "    row, col = neg_row[:n_v], neg_col[:n_v]\n",
    "    data.val_neg_edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "    row, col = neg_row[n_v:n_v + n_t], neg_col[n_v:n_v + n_t]\n",
    "    data.test_neg_edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split=train_test_split_edges(data, test_ratio=0.1, val_ratio=0)\n",
    "x,train_pos_edge_index,train_pos_edge_attr = data_split.x.to(device), data_split.train_pos_edge_index.to(device), data_split.train_pos_edge_attr.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_edge_index, train_pos_edge_attr=add_remaining_self_loops(train_pos_edge_index,train_pos_edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    26096\n",
       "1    21227\n",
       "3     1844\n",
       "4     1228\n",
       "2      217\n",
       "dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_pos_edge_attr.cpu().numpy()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,train_pos_edge_index,train_pos_edge_attr = Variable(x),Variable(train_pos_edge_index),Variable(train_pos_edge_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define VGAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_VGAE(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, isClassificationTask=False):\n",
    "        super(Encoder_VGAE, self).__init__()\n",
    "        self.isClassificationTask=isClassificationTask\n",
    "        self.conv_gene_drug=  SAGEConv(in_channels, 2*out_channels, )\n",
    "        self.conv_gene_gene = SAGEConv(in_channels, 2*out_channels, )\n",
    "        self.conv_bait_gene = SAGEConv(in_channels, 2*out_channels, )\n",
    "        self.conv_gene_phenotype = SAGEConv(in_channels, 2*out_channels, )\n",
    "        self.conv_drug_phenotype = SAGEConv(in_channels, 2*out_channels)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(5*2*out_channels)\n",
    "        #variational encoder\n",
    "        self.conv_mu = SAGEConv(5*2*out_channels, out_channels, )\n",
    "        self.conv_logvar = SAGEConv(5*2*out_channels, out_channels,)\n",
    "\n",
    "    def forward(self,x,edge_index,edge_attr):\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        index_gene_drug=(edge_attr==0).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_gene_drug=edge_index[:, index_gene_drug]\n",
    "        \n",
    "        index_gene_gene=(edge_attr==1).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_gene_gene=edge_index[:, index_gene_gene]\n",
    "        \n",
    "        index_bait_gene=(edge_attr==2).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_bait_gene=edge_index[:, index_bait_gene]\n",
    "        \n",
    "        index_gene_phenotype=(edge_attr==3).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_gene_phenotype=edge_index[:, index_gene_phenotype]\n",
    "        \n",
    "        index_drug_phenotype=(edge_attr==4).nonzero().reshape(1,-1)[0]\n",
    "        edge_index_drug_phenotype=edge_index[:, index_drug_phenotype]\n",
    "        \n",
    "        \n",
    "        x_gene_drug = F.dropout(F.relu(self.conv_gene_drug(x,edge_index_gene_drug)), p=0.5, training=self.training, )\n",
    "        x_gene_gene = F.dropout(F.relu(self.conv_gene_gene(x,edge_index_gene_gene)), p=0.5, training=self.training)\n",
    "        x_bait_gene = F.dropout(F.relu(self.conv_bait_gene(x,edge_index_bait_gene)), p=0.1, training=self.training)\n",
    "        x_gene_phenotype = F.dropout(F.relu(self.conv_gene_phenotype(x,edge_index_gene_phenotype)), training=self.training)\n",
    "        x_drug_phenotype = F.dropout(F.relu(self.conv_drug_phenotype(x,edge_index_drug_phenotype)), training=self.training)\n",
    "\n",
    "        x=self.bn(torch.cat([x_gene_drug,x_gene_gene,x_bait_gene,x_gene_phenotype,x_drug_phenotype],dim=1))        \n",
    "        \n",
    "        return self.conv_mu(x,edge_index), self.conv_logvar(x,edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=VGAE(Encoder_VGAE(node_feature.shape[1], embedding_size)).to(device)\n",
    "optimizer=torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, train_pos_edge_index, train_pos_edge_attr)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index)\n",
    "    loss = loss + (1 / data.num_nodes) * model.kl_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n",
    "    \n",
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z=model.encode(x, train_pos_edge_index,train_pos_edge_attr)\n",
    "    return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3735442269693406, 0.48381118468779144)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DRKG's accuracy for comparison\n",
    "model.test(x,data_split.test_pos_edge_index, data_split.test_neg_edge_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.497380256652832\n",
      "Epoch: 001, AUC: 0.8144, AP: 0.8218\n",
      "8.610543251037598\n",
      "Epoch: 002, AUC: 0.8120, AP: 0.8191\n",
      "8.553342819213867\n",
      "Epoch: 003, AUC: 0.8095, AP: 0.8165\n",
      "8.661429405212402\n",
      "Epoch: 004, AUC: 0.8070, AP: 0.8140\n",
      "8.526747703552246\n",
      "Epoch: 005, AUC: 0.8048, AP: 0.8118\n",
      "8.448246955871582\n",
      "Epoch: 006, AUC: 0.8023, AP: 0.8093\n",
      "8.594985008239746\n",
      "Epoch: 007, AUC: 0.7999, AP: 0.8068\n",
      "8.522296905517578\n",
      "Epoch: 008, AUC: 0.7972, AP: 0.8042\n",
      "8.421606063842773\n",
      "Epoch: 009, AUC: 0.7951, AP: 0.8023\n",
      "8.426219940185547\n",
      "Epoch: 010, AUC: 0.7927, AP: 0.8000\n",
      "8.452146530151367\n",
      "Epoch: 011, AUC: 0.7908, AP: 0.7981\n",
      "8.729870796203613\n",
      "Epoch: 012, AUC: 0.7886, AP: 0.7961\n",
      "8.435208320617676\n",
      "Epoch: 013, AUC: 0.7869, AP: 0.7945\n",
      "8.512262344360352\n",
      "Epoch: 014, AUC: 0.7852, AP: 0.7928\n",
      "8.479884147644043\n",
      "Epoch: 015, AUC: 0.7835, AP: 0.7913\n",
      "8.582797050476074\n",
      "Epoch: 016, AUC: 0.7818, AP: 0.7896\n",
      "8.554887771606445\n",
      "Epoch: 017, AUC: 0.7799, AP: 0.7878\n",
      "8.457385063171387\n",
      "Epoch: 018, AUC: 0.7781, AP: 0.7860\n",
      "8.408349990844727\n",
      "Epoch: 019, AUC: 0.7768, AP: 0.7845\n",
      "CPU times: user 2min 35s, sys: 16.6 s, total: 2min 52s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, 20):\n",
    "    train()\n",
    "    auc, ap = test(data_split.test_pos_edge_index, data_split.test_neg_edge_index)\n",
    "    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Node embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "z=model.encode(x, data.edge_index.to(device), data.edge_attr.to(device))\n",
    "z_np = z.squeeze().detach().cpu().numpy()\n",
    "z_np_DRKG = pickle.load(open(data_path+'node_feature_'+exp_id+'.pkl','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the new embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(z_np, open(data_path+'node_embedding_'+exp_id+'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the torch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), data_path+'VAE_encoders_'+exp_id+'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 1\n",
    "#### Link prediction result. Format: (AUROC, AUPRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "(0.8317428389304241, 0.8537968625010853)\n",
      "(0.3735442269693406, 0.48381118468779144)\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(data_path+'VAE_encoders_'+exp_id+'.pkl'))\n",
    "model.eval()\n",
    "#print(type(z))\n",
    "\n",
    "print(\"link prediction for DRKG's accuracy for comparison: (AUROC, AUPRC)\")\n",
    "print(model.test(x,data_split.test_pos_edge_index, data_split.test_neg_edge_index ))\n",
    "print(\"link prediction result for SARS-CoV-2 knowledge graph embedding + general embedding: (AUROC, AUPRC)\")\n",
    "print(model.test(z,data_split.test_pos_edge_index, data_split.test_neg_edge_index ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk=300\n",
    "types=np.array([item.split('_')[0] for item in le.classes_ ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load drugs under clinical trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label\n",
    "trials=pd.read_excel(data_path+'literature-mining/All_trails_5_24.xlsx',header=1,index_col=0)\n",
    "trials_drug=set([drug.strip().upper() for lst in trials.loc[trials['study_category'].apply(lambda x: 'drug' in x.lower()),'intervention'].apply(lambda x: re.split(r'[+|/|,]',x.replace(' vs. ', '/').replace(' vs ', '/').replace(' or ', '/').replace(' with and without ', '/').replace(' /wo ', '/').replace(' /w ', '/').replace(' and ', '/').replace(' - ', '/').replace(' (', '/').replace(') ', '/'))).values for drug in lst])\n",
    "drug_labels=[1 if drug.split('_')[1] in trials_drug else 0 for drug in le.classes_[types=='drug'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPR loss NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=70\n",
    "indices = np.arange(len(drug_labels))\n",
    "if DRKG:\n",
    "    X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np_DRKG[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test,indices_train,indices_test=train_test_split(z_np[types=='drug'],drug_labels,indices, test_size=0.5,random_state=seed,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable wrapping for torch.tensor\n",
    "_X_train, _y_train=Variable(torch.tensor(X_train,dtype=torch.float).to(device)), Variable(torch.tensor(y_train,dtype=torch.float).to(device))\n",
    "_X_test, _y_test=Variable(torch.tensor(X_test,dtype=torch.float).to(device)), Variable(torch.tensor(y_test,dtype=torch.float).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self,embedding_dim=embedding_size):\n",
    "        super(Classifier, self).__init__() \n",
    "        self.fc1=nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.fc2=nn.Linear(embedding_dim,1)\n",
    "        self.bn=nn.BatchNorm1d(embedding_dim)\n",
    "    def forward(self, x):\n",
    "        residual1 = x\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x= self.bn(F.dropout(F.relu(self.fc1(x)),training=self.training))\n",
    "        x += residual1  \n",
    "        return self.fc2(x)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler, WeightedRandomSampler\n",
    "class BPRLoss(nn.Module):\n",
    "    def __init__(self, num_neg_samples):\n",
    "        super(BPRLoss, self).__init__()\n",
    "        self.num_neg_samples=num_neg_samples\n",
    "    \n",
    "    def forward(self, output, label):\n",
    "        positive_output=output[label==1]\n",
    "        negative_output=output[label!=1]\n",
    "        \n",
    "        #negative sample proportional to the high values\n",
    "        negative_sampler=WeightedRandomSampler(negative_output-min(negative_output), num_samples=self.num_neg_samples*len(positive_output),replacement=True)\n",
    "        negative_sample_output=negative_output[torch.tensor(list(BatchSampler(negative_sampler, batch_size=len(positive_output),drop_last=True)),dtype=torch.long).t()]\n",
    "        return -(positive_output.view(-1,1)-negative_sample_output).sigmoid().log().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Classifier(64).to(device)\n",
    "optimizer=torch.optim.Adam(clf.parameters())\n",
    "criterion=BPRLoss(num_neg_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss 0.8268296718597412\n",
      "test loss 0.6974716186523438\n",
      "training loss 0.8317307829856873\n",
      "test loss 0.6882096529006958\n",
      "training loss 0.8825471997261047\n",
      "test loss 0.681315541267395\n",
      "training loss 0.7795369625091553\n",
      "test loss 0.666007936000824\n",
      "training loss 0.7614212036132812\n",
      "test loss 0.6533360481262207\n",
      "training loss 0.6930181384086609\n",
      "test loss 0.6473618149757385\n",
      "training loss 0.6933445930480957\n",
      "test loss 0.632901132106781\n",
      "training loss 0.5595831274986267\n",
      "test loss 0.626295804977417\n",
      "training loss 0.5852183103561401\n",
      "test loss 0.6136143803596497\n",
      "training loss 0.5491039156913757\n",
      "test loss 0.6066694259643555\n",
      "training loss 0.5917964577674866\n",
      "test loss 0.599033772945404\n",
      "training loss 0.5989439487457275\n",
      "test loss 0.5822802186012268\n",
      "training loss 0.5844507217407227\n",
      "test loss 0.5726327300071716\n",
      "training loss 0.5846743583679199\n",
      "test loss 0.575442910194397\n",
      "training loss 0.5298371911048889\n",
      "test loss 0.5677968263626099\n",
      "training loss 0.5705760717391968\n",
      "test loss 0.5625852346420288\n",
      "training loss 0.5261291265487671\n",
      "test loss 0.5636575818061829\n",
      "training loss 0.5212178826332092\n",
      "test loss 0.5508697032928467\n",
      "training loss 0.5126888155937195\n",
      "test loss 0.5574444532394409\n",
      "training loss 0.5100215077400208\n",
      "test loss 0.5485069155693054\n",
      "training loss 0.495684951543808\n",
      "test loss 0.5535435676574707\n",
      "training loss 0.4956306517124176\n",
      "test loss 0.5376805067062378\n",
      "training loss 0.5124281048774719\n",
      "test loss 0.5559977889060974\n",
      "training loss 0.4757423400878906\n",
      "test loss 0.5321527123451233\n",
      "training loss 0.4796101748943329\n",
      "test loss 0.5647459626197815\n",
      "training loss 0.4402824938297272\n",
      "test loss 0.5630931258201599\n",
      "training loss 0.5178319215774536\n",
      "test loss 0.5665978789329529\n",
      "training loss 0.4906309247016907\n",
      "test loss 0.5423173904418945\n",
      "training loss 0.5778782963752747\n",
      "test loss 0.5461531281471252\n",
      "training loss 0.4943980574607849\n",
      "test loss 0.5502942800521851\n"
     ]
    }
   ],
   "source": [
    "best_auprc=0\n",
    "for epoch in range(30):\n",
    "    clf.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = clf(_X_train)\n",
    "    loss=criterion(out.squeeze(), _y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print('training loss',loss.item())\n",
    "\n",
    "    clf.eval()\n",
    "    print('test loss', criterion(clf(_X_test).squeeze(), _y_test).item())\n",
    "    prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "    auprc=metrics.average_precision_score(y_test,prob)\n",
    "    if auprc>best_auprc:\n",
    "        best_auproc=auprc\n",
    "        torch.save(clf, data_path+'nn_clf-temp.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.load_state_dict(torch.load(data_path+'nn_clf-temp.pt').state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 2&3 - AUROC and AUPRC for proposed ranking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC 0.8626562857797115\n",
      "AUPRC 0.1641371892221424\n"
     ]
    }
   ],
   "source": [
    "#Compute AUC\n",
    "clf.eval()\n",
    "\n",
    "prob=torch.sigmoid(clf(_X_test)).cpu().detach().numpy().squeeze()\n",
    "print(\"AUROC\", metrics.roc_auc_score(y_test,prob))\n",
    "print(\"AUPRC\", metrics.average_precision_score(y_test,prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_items_idx=np.argsort(-clf(torch.tensor(z_np[types=='drug'],dtype=torch.float).to(device)).squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result 2&3 - AUROC and AUPRC for baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit AUROC 0.8532218914586671\n",
      "Logit AUPRC 0.1914080121558748\n"
     ]
    }
   ],
   "source": [
    "clf=LogisticRegression().fit(X_train,y_train)\n",
    "print(\"Logit AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"Logit AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost AUROC 0.8407831463247081\n",
      "XGBoost AUPRC 0.15412129651277998\n"
     ]
    }
   ],
   "source": [
    "clf=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "print(\"XGBoost AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"XGBoost AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf AUROC 0.8462193725669797\n",
      "rf AUPRC 0.15919719230188742\n"
     ]
    }
   ],
   "source": [
    "clf=RandomForestClassifier().fit(X_train,y_train)\n",
    "print(\"rf AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"rf AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm AUROC 0.6969315319441264\n",
      "svm AUPRC 0.1586807066814274\n"
     ]
    }
   ],
   "source": [
    "clf=make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True)).fit(X_train,y_train)\n",
    "print(\"svm AUROC\", roc_auc_score(y_test,clf.predict_proba(X_test)[:,1]))\n",
    "print(\"svm AUPRC\", average_precision_score(y_test,clf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_items_idx=np.argsort(-clf.predict_proba(z_np[types=='drug'])[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the high-ranked drugs into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_drugs=pd.DataFrame([(rank, drug.split('_')[1]) for rank,drug in enumerate(le.inverse_transform((types=='drug').nonzero()[0][top_items_idx])[:topk+1])], columns=['rank', 'drug'])\n",
    "topk_drugs['under_trials']=topk_drugs['drug'].isin(trials_drug).astype(int)\n",
    "topk_drugs['is_used_in_training']=topk_drugs['drug'].isin(np.array([drug.split('_')[1] for drug in le.classes_[types=='drug']])[indices_train]).astype(int)\n",
    "topk_drugs.to_csv('top300_drugs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
